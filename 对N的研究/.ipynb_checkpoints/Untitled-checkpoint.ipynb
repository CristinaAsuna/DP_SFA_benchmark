{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786914f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.stats import halfnorm\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import scienceplots  # 导入 scienceplots\n",
    "\n",
    "# 设置 scienceplots 样式\n",
    "plt.style.use(['science', 'ieee', 'grid'])\n",
    "\n",
    "# Stochastic Frontier Model类\n",
    "class StochasticFrontierModel(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Parameter(torch.tensor(num_features * [1.0], dtype=torch.float32))\n",
    "        self.log_sigma2 = nn.Parameter(torch.tensor(0.0))\n",
    "        self.log_lambda0 = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def predict(self, x):\n",
    "        predictions = torch.matmul(x, self.beta)\n",
    "        return predictions\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        sigma2 = torch.exp(self.log_sigma2)\n",
    "        lambda0 = torch.exp(self.log_lambda0)\n",
    "        sigma = torch.sqrt(sigma2)\n",
    "        epsilon = y - torch.sum(x * self.beta, dim=1)\n",
    "\n",
    "        term1 = x.shape[0] * self.log_sigma2 / 2\n",
    "        term2 = -torch.sum(torch.log(torch.distributions.normal.Normal(0, 1).cdf(-epsilon * lambda0 / sigma) + 1e-10))\n",
    "        term3 = torch.sum(epsilon**2) / (2 * sigma2)\n",
    "\n",
    "        return (term1 + term2 + term3) / x.shape[0]\n",
    "\n",
    "class StochasticFrontierAnalysis:\n",
    "    def __init__(self, num_features, beta, sigma_u, sigma_v):\n",
    "        self.num_features = num_features\n",
    "        self.beta = beta\n",
    "        self.sigma_u = sigma_u\n",
    "        self.sigma_v = sigma_v\n",
    "\n",
    "    def standardize_data(self, x):\n",
    "        mean = np.mean(x[:, 1:], axis=0)\n",
    "        std = np.std(x[:, 1:], axis=0)\n",
    "        x_standardized = np.column_stack([x[:, 0], (x[:, 1:] - mean) / std])\n",
    "        return x_standardized, np.insert(mean, 0, 0), np.insert(std, 0, 1)\n",
    "\n",
    "    def generate_data(self, N):\n",
    "        x1 = np.random.uniform(0, 1, N)\n",
    "        x_random = np.random.normal(0, 1, (N, self.num_features - 1))\n",
    "        x = np.hstack([x1.reshape(-1, 1), x_random])\n",
    "        v = np.random.normal(0, self.sigma_v, N)\n",
    "        u = stats.halfnorm.rvs(loc=0, scale=self.sigma_u, size=N)\n",
    "        y = x.dot(self.beta) + v - u\n",
    "        return x, y\n",
    "\n",
    "    def stochastic_frontier_mle(self, x, y):\n",
    "        x_df = pd.DataFrame(x)\n",
    "        y_series = pd.Series(y)\n",
    "\n",
    "        def logLikFun(param):\n",
    "            const = param[0]\n",
    "            parlab = param[:-2]\n",
    "            parsigmaSq = param[-2]\n",
    "            parlambda = param[-1]\n",
    "            epsilon = y_series - 0 - np.dot(x_df, parlab)\n",
    "            return -np.sum(0.5 * np.log(parsigmaSq) + 0.5 / parsigmaSq * epsilon**2 -\n",
    "                           norm.logcdf(-epsilon * parlambda / np.sqrt(parsigmaSq)))\n",
    "\n",
    "        ols = sm.OLS(y_series, x_df).fit()\n",
    "        init_params = np.append(ols.params.values, [0.5, sum(ols.resid**2) / (len(y_series) - len(ols.params))])\n",
    "\n",
    "        result = minimize(lambda params: -logLikFun(params), init_params, method='Nelder-Mead')\n",
    "\n",
    "        beta = result.x[:-2]\n",
    "        sigma_sq = result.x[-2]\n",
    "        lambda_ = result.x[-1]\n",
    "\n",
    "        epsilon = y_series - np.dot(x_df, beta)\n",
    "        mse = np.mean(epsilon**2)\n",
    "\n",
    "        return beta, lambda_, sigma_sq, mse, -logLikFun(result.x)\n",
    "\n",
    "    def train_model_without_private(self, x, y, num_iters=3000, constraint=100, minibatch_size=50):\n",
    "        x, mean, std = self.standardize_data(x)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        model = StochasticFrontierModel(num_features=self.num_features)\n",
    "        losses = []\n",
    "        gradients = []\n",
    "        parameters = []\n",
    "\n",
    "        for i in tqdm(range(1, num_iters + 1), desc=\"Training without privacy\"):\n",
    "            minibatch_indices = np.random.choice(x.shape[0], minibatch_size, replace=True)\n",
    "            minibatch_x = x[minibatch_indices]\n",
    "            minibatch_y = y[minibatch_indices]\n",
    "            loss = model(minibatch_x, minibatch_y)\n",
    "            loss.backward()\n",
    "            gradient = torch.cat([p.grad.flatten() for p in model.parameters()]).detach().numpy()\n",
    "            pos_alphas = self.compute_alpha(constraint, gradient, model)\n",
    "            neg_alphas = self.compute_alpha(-constraint, gradient, model)\n",
    "            alphas = pos_alphas + neg_alphas\n",
    "            min_alpha, size, corner_num = min(alphas, key=lambda x: x[0])\n",
    "            corner = np.zeros(sum(p.numel() for p in model.parameters()))\n",
    "            corner[corner_num] = size\n",
    "            mu = 2 / (i + 2)\n",
    "            with torch.no_grad():\n",
    "                index = 0\n",
    "                for p in model.parameters():\n",
    "                    numel = p.numel()\n",
    "                    p_flat = p.view(-1)\n",
    "                    p_flat.copy_((1 - mu) * p_flat + mu * torch.tensor(corner[index:index+numel], dtype=torch.float32))\n",
    "                    index += numel\n",
    "            losses.append(loss.item())\n",
    "            gradients.append(np.linalg.norm(gradient))\n",
    "            parameters.append(model.state_dict().copy())\n",
    "            model.zero_grad()\n",
    "\n",
    "        min_loss_index = np.argmin(losses)\n",
    "        model.load_state_dict(parameters[min_loss_index])\n",
    "\n",
    "        return model, mean, std, losses[-1], self.calculate_mse(model, x, y)\n",
    "\n",
    "    def train_model_private(self, x, y, num_iters=3000, constraint=100, minibatch_size=50, lipschitz=1, epsilon=0.1, delta=1e-5):\n",
    "        x, mean, std = self.standardize_data(x)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        model = StochasticFrontierModel(num_features=self.num_features)\n",
    "        n = x.shape[0]\n",
    "        m = sum(p.numel() for p in model.parameters())\n",
    "        losses = []\n",
    "        gradients = []\n",
    "        parameters = []\n",
    "        noise_para = lipschitz * constraint * math.sqrt(8 * num_iters * math.log(1 / delta)) / (n * epsilon)\n",
    "\n",
    "        for i in tqdm(range(1, num_iters + 1), desc=\"Training with privacy\"):\n",
    "            minibatch_indices = np.random.choice(x.shape[0], minibatch_size, replace=True)\n",
    "            minibatch_x = x[minibatch_indices]\n",
    "            minibatch_y = y[minibatch_indices]\n",
    "            loss = model(minibatch_x, minibatch_y)\n",
    "            loss.backward()\n",
    "            gradient = torch.cat([p.grad.flatten() for p in model.parameters()]).detach().numpy()\n",
    "            pos_alphas = self.compute_alpha_private(constraint, gradient, model, noise_para)\n",
    "            neg_alphas = self.compute_alpha_private(-constraint, gradient, model, noise_para)\n",
    "            alphas = pos_alphas + neg_alphas\n",
    "            min_alpha, size, corner_num = min(alphas, key=lambda x: x[0])\n",
    "            corner = np.zeros(m)\n",
    "            corner[corner_num] = size\n",
    "            mu = 2 / (i + 2)\n",
    "            with torch.no_grad():\n",
    "                index = 0\n",
    "                for p in model.parameters():\n",
    "                    numel = p.numel()\n",
    "                    p_flat = p.view(-1)\n",
    "                    p_flat.copy_((1 - mu) * p_flat + mu * torch.tensor(corner[index:index+numel], dtype=torch.float32))\n",
    "                    index += numel\n",
    "            losses.append(loss.item())\n",
    "            gradients.append(np.linalg.norm(gradient))\n",
    "            parameters.append(model.state_dict().copy())\n",
    "            model.zero_grad()\n",
    "\n",
    "        min_loss_index = np.argmin(losses)\n",
    "        model.load_state_dict(parameters[min_loss_index])\n",
    "\n",
    "        return model, mean, std, losses[-1], self.calculate_mse(model, x, y)\n",
    "\n",
    "    def compute_alpha(self, corner_size, gradient, model):\n",
    "        alpha = gradient * corner_size\n",
    "        corner_size = (np.ones(sum(p.numel() for p in model.parameters())) * corner_size).tolist()\n",
    "        corner_num = np.arange(sum(p.numel() for p in model.parameters())).tolist()\n",
    "        return list(zip(alpha, corner_size, corner_num))\n",
    "\n",
    "    def compute_alpha_private(self, corner_size, gradient, model, noise_para):\n",
    "        alpha = gradient * corner_size\n",
    "        noise = np.random.laplace(scale=noise_para, size=sum(p.numel() for p in model.parameters()))\n",
    "        alpha = alpha + noise\n",
    "        corner_size = (np.ones(sum(p.numel() for p in model.parameters())) * corner_size).tolist()\n",
    "        corner_num = np.arange(sum(p.numel() for p in model.parameters())).tolist()\n",
    "        return list(zip(alpha, corner_size, corner_num))\n",
    "\n",
    "    def calculate_mse(self, model, x, y):\n",
    "        with torch.no_grad():\n",
    "            predictions = model.predict(x)\n",
    "            mse = torch.mean((predictions - y) ** 2).item()\n",
    "        return mse\n",
    "\n",
    "    def run_experiments_parallel_fixed_hyperparams(self, N_values, epsilon_values, num_repeats=100, constraint=100, minibatch_size=50, lipschitz=1):\n",
    "        results = []\n",
    "        for N in N_values:\n",
    "            x, y = self.generate_data(N)\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "            args_list = [(N, epsilon, method, x_train, y_train, x_test, y_test, constraint, minibatch_size, lipschitz) \n",
    "                         for epsilon in epsilon_values \n",
    "                         for method in ['mle', 'without_private', 'private'] \n",
    "                         for _ in range(num_repeats)]\n",
    "            \n",
    "            with ProcessPoolExecutor() as executor:\n",
    "                for result in executor.map(self.run_experiment_fixed_hyperparams, args_list):\n",
    "                    results.append(result)\n",
    "        \n",
    "        pd.DataFrame(results).to_csv('result_epsilon_fixed_hyperparams.csv', index=False)\n",
    "\n",
    "    def run_experiment_fixed_hyperparams(self, args):\n",
    "        N, epsilon, method, x_train, y_train, x_test, y_test, constraint, minibatch_size, lipschitz = args\n",
    "        if method == 'mle':\n",
    "            beta, lambda_, sigma_sq, mse, loss = self.stochastic_frontier_mle(x_train, y_train)\n",
    "            y_pred = x_test.dot(beta)\n",
    "            test_mse = np.mean((y_test - y_pred) ** 2)\n",
    "            return {\n",
    "                'N': N,\n",
    "                'epsilon': epsilon,\n",
    "                'method': method,\n",
    "                'beta': beta,\n",
    "                'lambda': lambda_,\n",
    "                'sigma_sq': sigma_sq,\n",
    "                'mse': test_mse,\n",
    "                'loss': loss,\n",
    "                'hyperparams': {}\n",
    "            }\n",
    "        else:\n",
    "            if method == 'private':\n",
    "                model, _, _, loss, mse = self.train_model_private(\n",
    "                    x_train, y_train, num_iters=3000, constraint=constraint, \n",
    "                    minibatch_size=minibatch_size, lipschitz=lipschitz, epsilon=epsilon\n",
    "                )\n",
    "            else:\n",
    "                model, _, _, loss, mse = self.train_model_without_private(\n",
    "                    x_train, y_train, num_iters=3000, constraint=constraint, \n",
    "                    minibatch_size=minibatch_size\n",
    "                )\n",
    "            with torch.no_grad():\n",
    "                y_pred = model.predict(torch.tensor(x_test, dtype=torch.float32)).numpy()\n",
    "                test_mse = np.mean((y_test - y_pred) ** 2)\n",
    "            return {\n",
    "                'N': N,\n",
    "                'epsilon': epsilon,\n",
    "                'method': method,\n",
    "                'beta': model.beta.detach().numpy(),\n",
    "                'lambda': torch.exp(model.log_lambda0).item(),\n",
    "                'sigma_sq': torch.exp(model.log_sigma2).item(),\n",
    "                'mse': test_mse,\n",
    "                'loss': loss,\n",
    "                'hyperparams': {\n",
    "                    'constraint': constraint,\n",
    "                    'minibatch_size': minibatch_size,\n",
    "                    'lipschitz': lipschitz\n",
    "                }\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "231dd0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 seaborn 绘制 epsilon 与 MSE 的关系图\n",
    "def plot_epsilon_vs_mse():\n",
    "    results = pd.read_csv('./result_epsilon_fixed_hyperparams.csv')\n",
    "    \n",
    "    # 排除 epsilon=0 的点\n",
    "    results = results[results['epsilon'] > 0.1]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # 使用 seaborn 绘制三种方法的 MSE 随 epsilon 变化的曲线\n",
    "    sns.lineplot(data=results, x='epsilon', y='mse', hue='method', style='method', markers=True, dashes=False)\n",
    "    \n",
    "    plt.title('Epsilon vs MSE for Different Methods (Fixed Hyperparameters)')\n",
    "    plt.xlabel('Epsilon')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend(title='Method')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fbdf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 示例用法\n",
    "num_features = 3\n",
    "beta = np.array([1.0, 2.0, 3.0])\n",
    "sigma_u = 0.3\n",
    "sigma_v = 0.5\n",
    "sfa = StochasticFrontierAnalysis(num_features, beta, sigma_u, sigma_v)\n",
    "\n",
    "# 固定超参数\n",
    "constraint = 60\n",
    "minibatch_size = 50\n",
    "lipschitz = 0.5\n",
    "\n",
    "# 运行实验\n",
    "sfa.run_experiments_parallel_fixed_hyperparams(\n",
    "    N_values=[5000], \n",
    "    epsilon_values=np.linspace(0.1, 1, 2),\n",
    "    constraint=constraint,\n",
    "    minibatch_size=minibatch_size,\n",
    "    lipschitz=lipschitz\n",
    ")\n",
    "\n",
    "# 绘制图表\n",
    "plot_epsilon_vs_mse()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
